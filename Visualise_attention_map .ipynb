{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a9946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from utils import *\n",
    "source_folder = 'source'\n",
    "\n",
    "# Set seed\n",
    "seed = 1\n",
    "batch_size =  128\n",
    "# Define mean and std from ImageNet data\n",
    "in_mean = [0.485, 0.456, 0.406]\n",
    "in_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917a5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litdata\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class ToRGBTensor:\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return F.to_tensor(img).expand(3, -1, -1) # Expand to 3 channels\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "#Loads in data and returns the dataloader\n",
    "\n",
    "    \n",
    "datapath = '/projects/ec232/data/'\n",
    "\n",
    "# Define postprocessing / transform of data modalities\n",
    "postprocess = (\n",
    "    T.Compose([                        # Handles processing of the .jpg image\n",
    "    ToRGBTensor(), \n",
    "    T.Resize((224,224), antialias=None),# Convert from PIL image to RGB torch.Tensor.\n",
    "    T.Normalize(in_mean, in_std),  # Normalize image to correct mean/std.\n",
    "]),\n",
    "nn.Identity(), \n",
    ")\n",
    "\n",
    "# Load training and validation data\n",
    "traindata = litdata.LITDataset('ImageWoof', datapath).map_tuple(*postprocess)\n",
    "valdata = litdata.LITDataset('ImageWoof', datapath, train=False).map_tuple(*postprocess)\n",
    "\n",
    "# Make and return the dataloaders\n",
    "train_dataloader = DataLoader(traindata, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(valdata, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2eda406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Visualization of dataset\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08ca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "image = images[0]\n",
    "# # Convert the tensor to a PIL image\n",
    "to_pil = transforms.ToPILImage()\n",
    "pil_image = to_pil(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80efa3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The number of classes in the dataset we want to finetune on\n",
    "num_classes = 10\n",
    "\n",
    "# Load ViT model with pretrained weighs\n",
    "model = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b6e9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomViTFeatureExtractor:\n",
    "    \n",
    "    def init(self, image_size=224, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.image_size = image_size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "    def call(self, image_path):\n",
    "        # Load and preprocess the image\n",
    "        #image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Add batch dimension\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1fedc824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEMAAADnCAYAAABFR+uLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAstUlEQVR4nO19zY4svXJcZFXPnE/6BNmCZMBLP4LfSyu/iAC9mpZeG9bGgAwJuveemS5SCzIiI9l9uk/XGJABnQIGM9NdP2QymRkZmWRF7x2/jnFs/94N+P/p+CUMO34Jw45fwrDjlzDsuDz68r//7d/147fA/seO6MDxHji+Ad/+b8fxHmjfgN//seH7Xwb+5b8B//l/An/4rwF0IDpw+VcAHeg78PkX47Pf/k/Hx18G+gZ8/FXH7/8L6Fvg+jvw9s8dfQ/86a+B/QP4/X93fP9Pgf17RxzA518EAKBfgL6N+24fAAL49k/jnOufA+0tcPlDxx//S+DbP3X86W8CcQCXPwJxAP/w9/8j7vX3oWbEAWyf85/ZKWzj7+3oaLt9DqDtQFyBaADaaDAwfkcf56LP7+d37RLoO9CX5vUNON7GdWHXRJ/nxmhfn20YDQZ6xDgXwHaMz2DogW19WRiI7BCQnYxmDWod2xXYrrPDvGY2NvrQKhdKu+R92dFos+EA+qVnZ1p2pkcKjc9An3/3vIfa3Ee7+BnPOycMWENnY7LDMRsS6DE7GEPyfPAY8Sij3mNoWxy1Y33jqHbEMb5o++xw2GBQSD2vD+vgdh3TZe00+8F7nBJGu+RNtiuk0tF7quEdwVG9sc3reJ51LDoQRx8Nn/fskTfk/SlkzMGQ5vVxDv/X6K9Cts+8vS8LA5gCmHMtrjYStAP2kO4N7pDtKNe4Bu2Rn/M+a4NjCpn35a04bea10VC0omj1V6eJHsSfGJbcG85pIzV0IzlHuzTQ5v+wK13X9T3mdMnzO20PbYppljq4pWDaJVJg8zMKUob4jDCk6rzxhmHQaMk5gh2yG40jHuPcHjEE6POeQmymGQDi2mcn+9Cm485ozs60PdtGDxbTy/WtTldE2pn29uP+PsQZHCFOkTI3adzm5zJun0gR+9yN7ATvHaZxanjr2D6He+x7ulQOaDSguwdCHe22x7Aj07a1PQX47HioGdsVGqHtWh9O9xqtp6Dmz/FWDVeZQrgVDG1B22NoCoVz1LZQYPxpcyg1OBiagZbnA2NquxafEkYBKM0a13Nk2x6prsQFsJEzkLR2hoY0z+9TE7tQpg7HF/P3doweuGZSk/nZ8FjZlkfHc5zRc9Tbe95wNUZ6mM1PdcLBEo0jtcXcMwU0rOGcJos29agCoXfz57s7loZaW04JgzcU4lzcaCujCs3z4oVMZYudoO83FKlO7l0eqQArG1lpnT+PQnBNPFJwQJ0+LwmDSJE33q55BadHtBGjcMRoX4RNNrP8dHOEz9e0K4pbrgOBSuuW6UFBdJ8eJlRppmmHhwPtgct4KIz2hpwWR44AESjhOAKau+rUYW4Zea2MpmuFw/UtBdHX7xdtoYH3c3qE2rxdIe1MSZ4URnGNe06ZG5c4VTH6tNycPtOy3wU6Zi9KvAOgXbrgP+9bIk+zGQzGqC3b0TOKNnyja88KI47ROQGlNWDTidCosYPDeAW2I6PWNJDjdzOXJ08VQLQoIYAESuMZ2YZw7zG9mzRzMdDADwbmZ4VRBLOlS9s+MEev185xLtNwufUPu88+Be2BV+uzY108RTNOg52JY9oF5H3Hd33gDNTBae+pLaenSXsH0BLv933c/HgfHTnezbAR/x/JPrmFV8PNa2yfs8Fzbvc9sH12bNcY33XzRFsKv0+jTC+0HRRq5MC8z/DgDdj/NBpQoudXhUEjGFPa7GS00RhvKGjZN8P/RJ/LlFKcsLBU23Wi2X14KKFbQusLp567YdTgjlPo6kJKB3B+mrRqhAocVyeiGFQndnQY6qShS+Obak2CR5fRMxgLRtcog/kJ4zq6ufx53uLmHwnjcaC2mXsy0DUCoNA54+8+OwdsNscVUrv3IUgDKkboQMh3T2JpeigwNlqmHZ852hLD5bc8ob1PDZ+hwvlpQkhsUFcB0TV1n+pcOIbZUfIVRZWJC1q6085hMQ0S+bsY0JW1UhzC2MYQcgFlT4KPx193NjjnJtnyNIJsZT7cWS7NWdMMCcsoAQdwfevpefY0pI46OTXbJe0Bp5ieaSGCFO6szXC6zl2rU3ztEon36SLvdNwNnPx/Q8EQ0fv0CDZNjLZj52h4q+2yZ5sm0ZiyD6ddK0eGqgy6VwvMxCxdQy5S02re3cP2Qsnt0/PY9FGHYB0zQKZrTfsKIHTjzKl9yfv2Bz1+Csc7447dAjekUcqGdj1MAdQSsVL125vhBudAqPp7l/Y4kXvjQm1aFqJpsRmcdoUlf1kYbCDD5c0axPm+owjhxrUugRjJlns8QyLJyAhz9UA9/y6eiB2yKZGETwZ/jqrX46FrBW7VqlD1lhaMI4Deq4GiljCKnGCrkEJHTm4ZO9wKSlk4Y7TyJjwnSuIpJrATfF8CvvV47k3UqbCRM7Vv2eHCL8yO+ki0y21qse8JoRmfICx9MDVx/+w3RtM9BWCejRhnam176089yXNhzA5g+nC6uG0mpDezH+IaOH/ts2LcYNpiBs05SgVuvIexVYxLPFbCMggkgAXD35Bs/mlvQgPIkZfR6sIdGp3FQMmW7AmhmaDmkbmRUOOJWNtbL/cf6HJct3/YINi9lIGf2kPGi15unaIvCQMwtssgedtDDWEIH9ZRRqK0/CprmI0W7Wf4QrTg/L19xpgaUaPNaBYtd5ROarrOgWpvVYsQwPW3k8KITneaVofxggTzNkbj+Jba0t5TS1bNKS4zckQVwXa631EQQ9tEnoJtUtLbgkO/9/Y5fi5/mn05xgBRq14WBkeAKkp7cWOR50hy3nueomADpK0oRSgwwzhBUrRQeYHbnHYxm2AMFzWD9AGfe7ybzblTFPPTwtg+Z8Nbz5uwYZZC6BvQLznHk6ILace4qLraQuMZ7tg+8jnYZue3216U9IUh39UukLr0MOFlYQA2Wlsiuuh9CMrijLhGaRgRIXlUkUJ0rQ61e1IFXosBwIKwvI4CVLUQTJh+fVSt+RICXeufNiHHKNnsQa/19CQWpfaZ+JWB4+ixkWZPtiUNSC/jIT2QGusBo2vg9plaU0CjxzmvCkMjOV1Wu0CeYM2CbUeCJ02fmUQWLwEkUCvwuxf4HUekRjjKpcfi6NI+KVXRNRjqv4cCDwTxVBjdpoWDIFlvs9x96wnCOPL0AB6IAam+0yXrHN27G18yzjvex+/2bmUNHh81SxM0iCzyQLC9f8GAFt5gqZrL4CxKPOGlRXSZrhXbtQoV1CATCBGlE8kigT1kd5Tp6HI+n23cPtOQb2enyca86YxSqeLON2yGQXoYwIrUKB1CmWbMPN6ZI7x9pjFeBSx3vESrYrOWKUJPt0bPLwujRHl9+RxG9VtMICDkIbxVDBa3OhssanGy4X3rJeKNYwRqJY6hAO2ce7hl2JTQM08nnsVWmbV2rkKNYU7CjZt7DUOcq0CZa2FGnwZzrbRplyiGV9OCbtyMeimqWd3pChh/WhhURVM/JnnJGyi+mIQMz6PrFUGM6lXcHrAW1IMxxw/eSk81FgrRB8xshwOwAvheFUaZ1xyhN7MdzfIikVQd+owB5nVO/a0NHuh2/LN/TKHMpI+IGvNGnrslhciOM/XoKYbRj+RHTmvGcJlzLpsL84Bo/5zsVk/NEMLEMq8NYQps7TFZ8TSsxA47s2XsuHGe1EiPlDWVLF/i05PZtVPCWNHb9mHaMjvsrnNbjJiml43oZqH9aGBXZ1d36nZI3IbhG8/Vyn1v+Xs8M8oU/hKfwSod8QwfORUcPCH6TDvmyKuqZzVwPX/aWwyswiIXjqhfh2Wum90peZMFlwCcOv0pxngujEWSfTeuAvW7aJnwLWH34tsVwpORmpSiSCAzfl4nSpKmdHox7iudGMvvG4/2ijBKBoodNUNUYPEycqr7csyBPN+Nc5+Z/EIf9mEz1FBCfU8RzGcVAVMoy3QoYO2MMNoFyR8CAkj+AEHvvZppX2tSLDtV3FXWscrR1VkBJHPZ3lHZtFbPd5s1/g/9/6WoVVHnYqE5hThiii5jVvxg/E1I7xpScrEEUXKJoWlWjO20D9sVZUDcXdM1e8Hd8DJDKu0NDwvpn4KukeuIGggB8hB86PYRabS+D5xBMLVWzZSwnI3v2TFOk2BxnNkI0QhIcke4ha50YpTjPdvcrArxnDBIuV97IUwIbkTtm1qHq+zUBhWpETxRUwxArSizX4Drn4WEV+Y7bdbkQjWdemohDXnbY9SIXYcnPE0IF57AGn39NlymciCBEVxxFBdrn+Btnt7yZ/vsAl1+TVxNozhND6r9bApzOmboFb2Kasx2tbcvBGqqwEU2Spn12cD9oxXAQ8YamBiCZQcrHph2o+8hOK4cygzyKGxfMuqEs0P18X3ovvv3bHO7DKO8f1g7XhVGEjm9GE5nvYYazr7Y3B3C7MXlAVDNBzuwstt9i5v5Tc1im7KB1cjLe5Ds4TM9pnnQ46cG1MNh/q3G8gEBFb8rv3LFjDvqLW86ZVPH45lYcIVX4ijZ7H/zeyuV5uDENXTto+Mpn8FcqLiMQJYbAmKwmVGjVxilAF1R7Q0Pas+AG+Md2K7DJrU3c+ecXpE0ggNBR6eF4KFWznjKU50vCSNJnKHuHiT5Q5wDJfbIsoGe33WzPXZtBnq9zGkneDay4A7NF+gt3nN2vMQ18+/2wL0+Z7qAwkYpQTO/Ywmko0JNhT1uIPxoEcr08NytZ+mURJ64oRvCFNibdiDztvMcljP84LkvC8OzafqMLLV3xhBjSQ5RpuZtqP7FDdPAWYm06sqpEdN+qU28j00fn4rKw8A8nJ3zsjDaogW84XaF8h3HW4qatoKlQ1Rh9zZAznnHFoo290DfelnawWu6a8BmUN+my3ZUnkOE0TTsj3DG05ouXyoRx7hi0O+1YSqaNQQ4BJhBkqsoG+7TKhrmmtUAmhFHNMg2nbZZYlCqgOfgKCwQp5GCOa0ZVHm5yimEu3WZNi2KkbLpJLbK6zXMIyTR24vKU3gipWeWTOUHl7y3ns9ptcQ955muCWE5/4T3WcdJms84S2ICqWOzjtCbGHHsy7YoRGbA7rVHGmeZfS/NFIINC9TMg53XDDNe+mwxaptxmAqczNZEywW7NxT/VjWKHe6XnirNEODI6xicOdnMDL5c/1JluF2HUf5SCE+j5QjQucftmmvQChvGKrzZGV3bzNKbHXF06GUIvhSzJK6d0zCX6sSxHwzp18/9eL7exEocua7seAeO3waG+PzzDX3LB8UB7IY5jm9RvMdAmOP/6zSCaUNG2mD/Htg/Zua9pRYoRukjHG9vwNZSaA28Rwqx7YH9ewozziLQFTpzehTXSA0xo+qh+upFGJ4XTZD2jVUFTAPIHth5JWCETRVDvYUda70g5NOgiwSKq7SwwqzY1SIc7zQF0sb3ZX0pwZK71DnnS5j/g07TraqJhinItK9pheK6zwpDscanRaqEyOpcchH6jiPIYtalYs9tCrnUAvAszuBvj4VE/LoQOqfhaM8abdN2Pcq1PrUZQM6/uAJ9Vr+wREn0nxelLXGKV/Q4mtUCHEei6IieiwCVc3XjvGKUqcFD6BnshQnUs/Y/Oh5PE5j0TTX3jzkXl84rZvDyaC+ApVdiw3jNlp0CoO1qHPR5/TgCuYubXU9o3t6ialnYz3nNiNyVDbWxQI0DCL68coeNd6zS3pBZ9TmSXuPZI+TBqHHbgkVowyjY+Ex3zYplJ6x9IM/nTWb13N0YIrIsUSNLF2fl0quNUUFLM5uyxi2XFPINmGo1aAvCcp7LCFcET8+VkT4l7xzPA7Wexk4JZUsGUQjHe3oeht+cp9tHx/bNFvvz889REv35eyyZ+RQCJs4Awdec/76QJzDa5zGR45nOWbMXT31zPNQMzlXxBhiNUzpvn641kgrUTkg254/fcv8tJ2uP9472lqCsvYe00XMq+/dZEM9p0fN7kda2qoGlDB5DAY+nyFNhOPmqLZ7mZ2NHgplDmYdWL9IVmpVXmL/EFDKsx8AuPSK5iiMNZdlh0ryIE8lrjzRlev05JYyi6oYwFXFi5DeFOczNFmaJXqTbKHmQNZNBSlaZRyqhtyHNvkERcfFORgazrQwgH0WsT4XhrJCM10eVsAzgbJwaOzVp++g5b6lZE0kKj5AZm3ajzbrNdkkSxyNe1ZFaFDs0shdBrwUvvnnJy8JQY8lBmGdw0tZVVZ6D5zJVYMaRo+wRpCB/n6kCmA24WIdJNi2JomE7ooCuwri58T4jDFXsMnnkNVTcH8eEc7PC2dxe4VEJka1Dmsv8nv/f69jUQgAFyHkYwPMZI/F5p3GG51qB9BTDuHU48BExCxuxALbv1hkzfMrOtyqUgSD7YKmQBpO1GuItFlBAj+QJJD6X5/btC+TO8RYJgAAlnWkgo5tm+NaYbIdV4XhGzEfLo8ptnh8tstRx2R3Bl3j44RVCq6EkKhZmOiMMrjgSuWMF8HSBK+ZXjIA8R9TeGp8At4kdG92xQdG4saoIzXMI9doUau+RcROFYlPu9A5uehjBzWJ8iruyByZC7QOU2TmbEcoO75UWBCTgwWxF6cAasMGEOxb42da75sF4zfHtpDCEAi0k7osWOOYo/l72I/JaU3cROg6SJjnD61WaRGRr8U5JIaiz06gzhTCns9DwlzSjLyPvSI6st00B0vUU4oggsy7LESc7wmvUsS3LoqJBFcQeGcuo01iWe0SmORm/2LZ3j47HBvQ9ywm0UrAT9AS0oY9jDosyxxSL7GhkZFnU3Tu7AKfjW15PA8xAEBtqSsEON9i8v/OzLwtjm0XyWsk8jZhSiD0F5PyiL/K9p9ZCqNbYZNVsXcmGUrUjoTJO2UztbUr3HYL1zrOW818Vhlznlp0A5ghzNbK7Kpvj40TUEZkddw5TmmSjy/+Ja3QP045MUqXWuWa5oH1ZyOlVBR7sAKj1WICiV4GpiRtIHvfLwA4+dYo2zOoeAGKl/B0H+YzsuAMrnWNTFdaWNYB0T3fveEjuOPzOFAG/i6mqScyIf2CtZR8dVE3olqp/BAZd51ttLzvSi/8oYUBOWT4jkAg4OtAMmo/GmuY9MKIPNcPnc99r1MeFuaU4ZeESilrSM8UyOmvAZ/jhdgU0NOWYOy0eKcYgyZ0y4WXs/KMw/uemiamoOrDnKmj3/6WTe17j890bh+V7bqiqIrnWSxtkuwygCapbu91b3atAelkY61FAz4wjFCB95iiI9jPV1fWmRVrubWRy3wJMWG8HtNSKo88p2KiJhl/EaZhGt71O49MGVMyUuU/AOnAAsDUlzJloBCINJA9PE5akUuRzuPxbcYyNupM2Yucdlltbh0Az1/olputm8Y1iADvH0aUx2MpumRr79zKoFgEPS4hCBq9CdKLZ4xnHOc5uAaikzlkDquy1G0eb41x+UYIsG+lRrJaxiXIfa4MWzduumQhSKxl0EZDdCRp5RKtpAweB5wlhdcxWFLn1X1ClYgEPy3sliTwuUdHcnU75CsUKqtLLeDs4JZpVH44iuRwsca1nhJGNqEsgfGQkcWfPDXPQBRd6b/6MSuLc2CxRYk/X6rZqCt8Nr9MLKlQxsLV/9sKZnF5iIQsfy1a0zVzdCsGnER2epd/M5YIuY9y7Bno98zVI7WCmHUCiS2uP0PA0mE4Y+9KM07HJSt6sVb+eCJL0e2bXWIWTF+UUWkc8g7NQh0v0y7YYwCqlDAe0lS/xD4VJChH4AgeaEWkvXkAMk7vWt3SZ3AKP9/BKm2ZcKcsX19d3bNfc9VGYpZk3c0xj0akK8enNOKUvuSz0tM0onaeAXFO2FNh2Z5UPO6iNzoDqJewzABmnRP0cxAsET56emN8PbZirp/fqdkE0+5Vcq1Cb7fgo6l5wd875JUJcw/PCdVBoiztmEttBF0DKL1JIhn8U/8S4ngy7A7HCrC0u+eeFYaGvNKTVztBIeqrPA7SVSC5+3ryBGmvnuecpuMLwx71o9F4hL9v7CHQ9rs+QT89FdEAaz8JVWhEbO8pzwwS4ksylwUSmUbUCQAGAcvm8WO416jPu9OdRsPbcZizAin8ny5TbVzo17wUtCpy8kdMbSIjECXNhr3sRABXQrVDbPJPY9zu52EdT5LkwTK2KmrKzgTLSCaftvDvTRB3yAIzgLsZ+Fw7cnBi6iUW6CxNyzT4ADsdPb02FDoXTMkQczTlSqh1nY/fqPXDnt/Oh8BHzqbRML3awGN/FPhG0OS0gHnU+6/xuTAGBGXXAUB9JHnZAMtxs1BbPIaLFBbJ6l/k3VzXlMnR7BnEO8je9UUG9i/Y9Oh4KgzTf+gIVIOequ1Q/J/Ma9pYL5JxWAzf7bAqKc160nwkXyKlW4qUtEajuY/fzMoVTwvD8hiLR+eNcqJNA3nE1bLHi3nk3oOOGQN9zvy4Z08UQ+6E26qVVEG3owva9QF4XRjOBOHBBSliNbNkIfTah/BqclVFdO9hqbxW0UaCcZq5him1sny8Gk8vgnHetAkW36QBvLDVCU8gE4RuYroiQHblLulinYVpxFzjJYOZWFgWvLIL/0fEYdG3Ma/YicY1WAFxVIPpucV2+lFJeouf91ziFz7iJT5bOA+bSDxS36lFudJseX7EZzSNGTwNydMygeScTnEW17qa2ZNS9lNExQmm0uVpgcbt0zbJlWX6tZNMdvPOyMABoMS9D6jSIkQ+cW7+UFT+Y55FjcKzCrye3KgPXOlhC6aG4s+F5bj6DAqLg+LnSGPayqC8QwnWv4BsVoyawVnNZ2UzrvrJdPKd0KKpmADn3pfbmnmWszZCXgM9A4LMAjcdDYWzXDNDcwBW2ikVsQDGuDJyKh9GX2aEb47kgTmpeyYn0KhBdN432ep81Gj4lDAdCatwkTrZrzyXZtAHzXDZ+FMjVxnGXtu5zXTZnnNQuPVOTgbqb9AK+gNSQ7bDouueUUg1ZfIH240EvUCC0CWwIJLLBBGGwDtj1a9oAwA34Kl7GPvOpemtkbZqt7fQ2/eB4bDO8sTa/mwVGx3vkP3auyF2O0hw9Lbc6agQpHrQDY2t+u+3iFm9eM9aroD0t0admie164FGeL/LlUgePH2bMQW3hOw5XKs5XGhU4T/WexSraRcmmnOKe+bzeoWxaXGfOeanq6dPLqQjWC91m205vMwNAi+eK6nXIFkRDvjhqzlOWOpfaLj6Qy7Kc7VqMZnsblYQ8lzZDRO/GcoW8XvnXlvZGa1suee/zWXhASyqA6spGJW4+oL1hvNrvytWN47zrnyEbF1NQpgXciiojzLH8e/vIkafW+XtOmCRiaC8qchtLvfyZHqCdRqDA7KjtnZPF7734di92kztufaxDNyPoLNg4t2fHDoxk9W4aiXG98rUG7TlAWpcyn6st9pDT1Q33KWFEg3ZwLIvh6A4Zi3SMxXd0m3tqU3m/6s0DRuf9LXv+nb/QxT0Vvy/8qSD7nL7Moq0I9SzOAMY0kf+2Q/WaO6fJeIrc5j5Hme8pAgpYKuSuGd6SPw1A282Q8XLjuhhkF5LjFMdBj7TjcbWfbTjmccHYaqYDLde5O6s0pEFp4gb1xfQMqgqaAKxoT0/Xu9aN+3NuSafQ/ZQ+WOD9j46nHKgk63DcNjt1TSjXzFEFquHl6Mv12j1Ktc1K3npcssQp3kExbzM40y4ORKlns/BEkqywBarPLkeDlm3qWtZ1rIjQBOWlDT76BR9QqCa0AqLonXhv61nhWJ5Mk4fC2D+6JFrQn4GolZgFEmX2CEWyK213wzFYp9Z8iY8oQ3qxWZbB554ebke4NlbB4Vdwhqu/vziFh2e/fG63uYpIq4LuTDcaO7ghtREvyW5OjWWKcGn6OC9rwQj9y+sNaad+cDwxoC6VRKLRgWYIUuG0Rmg25i3Skrep8Qahb4I5M46i7KZ9gb1ACrAQYU6lvk2ZWk72xpjDPrtzPDWg3L6WDShEDXJKAKgLY2YjFEg5GbPMcX6nwI6X+3O8yMU+p0CS6ktXfqONqxt+RRib7ZQEU1up2+LnCatZceO4Yc2MO4sNMCBMyy8NtM3SvHK4u2buaVei9czCNbNpbP9p1zqNJDczVB7VVHj/PhEfR8AM5WDVreCeWuK1GwuOGa7WkkgzECRkd1sSJoyhIV0vlPLB4/El0OWZKrcJNEqj8ZGCQ47SmOuRU8JVPFJwBX8oio0kcSes3oBqQ+a9NrtO1T02LbjTCtv4yGY8FYanCHzPbiajRapc6ojJbbobjBTwiGeWcwCtlY+pWcONTSV1MugeHA+WXXYJeiwlDVF+p0GXv1KwZNQ25OpkTnPbDaUZqmQ4zaNU6JErtfqwwUdQE+dI29xfsULfkNiHq5r2KBsfMfzfv+O8zXi0cIYh90GC1ea+733Tt7qd3BBQdkT74AQFF4j5tj5tdmZTlC7Zd6Ul8dPe5h7mjoYNq3ypXPp4j1wTGo4spz0wFW+XsZqYvEbZouYe5G7prViW6Jl2eYerbVbi8YjZD3WaQd9mn3doML7kWocb6+meOJI+92cj+4XDDo2UZ9t4cHNVusuYmqGRb/ldgeUGohQlI21MqQJwF83nPpgePyUMoGKBAsVXVNcqYCo0mwM1utag6+yqzCkpBAuwShtonLf8G0CuQzEv5eURzqCfEsbwFpG8wyKgoTH11WElxadG5juV1oiVrcj4YSnNBsp9+f/4IzkPddYEW1KYvO70NGnIMmW/whrr0hcSNDou7mhTGr5h8BzZ6rxOhitqGtIGwnM4Tibdg/HFXp0RRuEW1ogTKQhxCnd4CTLgHlqnyxtR7U1mzHgLcZp8tsP7hRnT78jfvs+HF9S8LgwaQrfmbivW/82IeWTq5YjrNALyGdQEZtvyjXuRFYfIKeDUIPOpK/VYNhpY4qOXhaE/Ddz46Plvj0Gqf0+06p1wnoPC9eXgx3voM6fvbiLbyMyfhMXBaCmIEef82Gg8D+Et+nMgVZjuoBtO3+4Mtk+FIsQf2BT9u2U5o9aeGdaQ0GdnfXMyaqe/3Xdc92Oj8VOboxZ/zc96HWm3FWKfFrB0o8LmFQrn6Z5jWR9fouP5zDAb5RB8DOLYbLUs5/zB8dSbOE/pnY8jYxfeqWSxKBSfNncM8E2sYSNN46ttIOCf2/1LcFcFxuCNiazzNgPQqChOIFDC+PvtD/P1nzN9wFdv+GJcLtYvTw2gcx9PFqPM4KzPvC13PhivB+gVWFErGixQgwjjmESwE8m+kfvrwmDsYVXC2mdvduh4mx2fb7HzzctkbD1qNasec7clpSQmQPM9vIRl+L4EN96GZAGIOPKFgr79bjPhvCwMvT5UIAll57QRe4RQallZbG7UjTACqpHwUgEllI3DVJwxBeqbAbBdLuRk4XqmFC6Rm6wxcDwjDBklGjjH/dNGjCK46Q731AJP3KwxSxHS1ATnNCloCucef+o2qJDN1rPodUC/ZEABaJsYMkVirCIbC6SqjifjrvVmFOtI0Kk651CjjynIWEVl14srFn8ijUycQsCmCLfZIL0sDPppC6/FYB2mkvawQuPROJqn4TnaIZpbVq7o0AVmHkSL9Bj7GN4ROwboHQjrlhVfNqAlPvEGAnPjw9zwQ6O3uDUXmMLzjixZ8Ok0BcMlnIXzYOcv2ba1s+vbdihMcR+nhOEPidoYeYUA1o0DgMXnHzAbYMKZnfLUgaqKHcHa9CibJk92jOmBJIx6Tmnf0/wrOEMErSNR0w56B37XLadCwRRXaIiU/GcYDnAb5Ay5Yh7iBA7GlnaoAD6/x9FVAfRIEE+FIXCFFIDnPArcZsfDRoDnmiEtW+2TEuy3oyYbEMu72ObzfCBWLWQsRAGVjUWWafXTwhiv/wtJtcy5zUZ8G9T8/r16BA+1KVAP1JiBd3skb+DxUGTCms/X+wys0yUmuYOJCh3xqjBU62nUv9huexfSNhnsbgISxHbAxqd5IOauEEMTPB8D5N/NdmIob9NZdlm4/pbThNOX1zzq8dPt+dft5bSD23tItd1byLjuQJ8CVEkkpwz9/cQBMG0aBjTMEKfNUB0oXTOFZdhh8B6Jffyd0Q/35sdPuFYvY/TObp9d2pIxAeSOmfdw7ShVOhOCu10Z59TVzu4m3WNxu+3VHZfmdyiz72370fH0lR68kdN57hUUPAESLT2F/qYAAIElD+YUUzQgtl5qQ6P1sgk7hbe+WW9dO89n3Xz24Ph5b0JVlCEaT/COeAJI7HjPxpdG0iMRU2z0ROwdB2EacNO4Gw0wT7OeT41Y3//4sjAAazSS1lMIP49Sc0XU575/s3uZfSktmFON9kShNj8Hcr2a2Se5czfKLOIlKHQ64DToisi5ypEy93Tjsx0FLuG8a4JHt7yOnb1d6t2VYRsfWKcNb7g9Khsf2SCtkfFLwuCmgYWP4G/UEcmyaRR4rfDa60c9FrHptB5lrru36jZtwwSyTRtDAtraWabzGWFQM9aOOwT30QcWF2bWuxC17gGWWgsvqic8p9cQUPM2zLYJaG2RtACWduG+0H9KGL6c2gkZvv2mb9PFuuulvaALXpZU0HsoyWTTR/Pa30eyTElp5USkrj2esCKnClQ7cfodz0XCtPhWYsD1ITdbNayv5FngeAnwpjdBYGxvNUGZrj26bJU8iRl1txnaAtyO5uTOA2LnqTCcopcbXFYCjNFfCkoWhKhGo3aIQZUCqneenL99j2FPHJWs/uqlzFjuLKFG2o1zwnAjpUAq7QTdWnsLudwbVtt5DwvawNB7NwJ4XzLy1qmSo2Gf3a1SWyxAXAOzHy4C+BlheOSpjnmcwnDZdl7xBNI6itqt0cCWV9y44XWv4aBNBJMBOk0DM8xsD4BkxdwAvyoMskiFEV+MUd0qO9WwvE/JR2fpsIfcQG2sv83TmXkJd60QQJ5TPJ4oRvv8zvHEtVYcQfWnN0hSGNXrmNGUwXTBTIaLU4HchO/tUwR3BzBFy6y7Q/qb65eY5jSfAZitsOy6E7Qewnt+VcKM9ARslNBsDCPN97uvDeX16qD9TcKn4IY7iFhTEI8F8VQYJWhi/OHeoM1XBUXNpW5XG/nIxksYNp16ZMbLRxowIVPbDlideK3HKLUePa8twNAF+qowHCCtRW5ZXtQLoy0vYVjknkGUW7QpJUAUKVSPcEuqYPEUubivF8EpcHwSlzwXxmb0HfKmozGR7oztn/8r+2WNWAkiCcqOe4Rv9J5LOf1csulz5LUT7CWqjaFnuxP6vyYMh76m3mxEoftQ3bAv59LIxpL4tbnsBtCFwh3p3dso50L36gX2G+B1XMXWPDmeGtBiqSM7fJe2dyF4gNTr+UVjjl46SuE4BJct8PtbuzyvOwYqqcP/p0yXM9JSM5O4Okv+AlWTNMd91Dusc5GkzSJQQXjLtZR7RgqLyHj7vH31uU/L06715kH++h+r3nEsUfIdC6u15mHJP/h9xne9eC7HOI44C3O1Yh2PQZ7rP4CfWP59s6vBAW3Ljz5d6x5obz1ZLjOs7a0aYUJjYRdYSD+fw00GVI60j9J0L7r1wI9xCI7Z5rnVt3K8Vq1zej9Qz27d8IycJj2ZpXvW+u5yTbdBEbkbAvJ3TiurGZstXpGm2zECQgHDZR+e03yGE7zEGVrU4kbPECfu2YfZcHEbCyLUSPcqdOZRPKZw5l15Wr+fadgwsv1GwKeEoaMjkV+3q34wd2+iRgNWtaH1MU4RyFZgsRFuAN1jENTN4pcUdOKhR5Qf8EQY7RLZSLoydtxGeY0StczhzlKoboJwckcCo8b12nHZFfcMbL2DwYtbcGgK03h/aZeEMuqzkTFfyuAELbNeBFxaauVw3KF1JE1HL8WQXTvUcrOijrQr1DYP/oh6m7Wr5/keu5zfQ9gEUFzmLAiJljfPKDQbxunlgvV7aoUyKYHPxU3veQ8nlxxqA7QtbEfkABk0kAAfTJUnBrRn6TLnq1H5voBP/VxRoeEDGbipKaT99P2eQnQuYq38lVBsaoa1y98tX0DfV2yGv14UqA/k582KWdR5E4qKU93QHXYvCoceC4tmzN8skeK5WsppdmxdleTM22ZQ/5QwdIa7yOVmeo2GIVA3sB66K7Q2w6cVkUgjSSEI8R7DJt1U4iytF6c6s/iFqaP2nJ0mNw8CsuOuwnRbvY74miSWABx6O2w2YHbTWdO4mw65RtpntBEFsp/WDPPra4qfB41beYhNo9IBj1MIp3u6Pk0bG0134bq3ufQCtO483zXiSzhjxAnZWCFMm5vaTuLSM+7gCoANZfmDtpwwm6HdaA13xHyvYpjmyP44dllwjvYgnLHKWK0d2D/Mnjzo8VOcET73IhuHRROixQ+DL9/e0rGDAzAAZXmEsMpkuvSoe7S/IczhkboZ0Z5s2AUlaHtZGO0Skj6r5Zx/2D/yfax6r6KpqfYWdi+zJz7RiyfXmrAJtBRo+RSbf2v/HrMJ7u3kjS64md4vC6PkWpGjK/DSR2rR0wXCA1OjChcKmzZ9aJMMKrWkLe9V6x1cILgKpUBr816+M70X1T6jAH/OtcIMFX/ovz+5+2PPTVLveALnH4pwwl6qTUC1M6nEziyWj50y76NpsUwD7nVcgN8pYXRor6uCJhlgbXOaNGD/bvWWlIFFoA6ItDH6PnpAhNn21DR12sN7W6OWHKq1aZZBecmTAsY7uOQ1YcTYYGgzOEyg1OerOo73mJ2AAJSXVRNZOsPEfIhAF6BdVlTgxrwHzHuZYNcsvWyO45o2vB3r1Ut0/aowoo1KYACqwBO/uLDeTtquhrAUxbFDXOhn7LjYrx23WXeiWgp2CrLENJwyDWWv4EeR6k8LQ2DKKD3PifiUEOt0yQ7dizABGs9stJc8eApgPBDFxozeIW2EBW3lnHkf7k/Ic85vqBx5lgATtcH8fKRbH+3gdLEf5yEE23umIpNM7qWicC2jdEHT/vi5Ko++MCpOwfB5p4RBzsI5ye0DBQKXXZ4XEAUbYeIGz+avhe3cRODmWKaYik+QWuaG8mZlFGHA/rjHT2zGUDEvEitxiYfl/NrYLSCNLg3fmnb0EHxA7p4dJBjzKebCdCQ7hUWqcmXOJbzzryiN6f85Qc3Nbtlpr6EgGPOEU51D88GzUW3hLH2nlb5BW0UUI4u0M2yXbMJ1biRggI8C/9Ly7+3aa4UcR+zA2DJqyxGkO21vwPGbjfI1G+qj3S5Z3ecVQCrUF+UXua0d3fdis5xf3a6ZuKJdOd6hvUxjmZo/LYy+DRsBVBUfgVSkpgAybHEgX167B47fxne+aE5GcVbwcaVh49IKwnoa4iOv47N4BD3SxEHHt0gudWrn/h2qAHikGdGX+OM/8vHTTNd/hOOXMOz4JQw7fgnDjl/CsOOXMOz4N7Bv6fWgN8gKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size=224\n",
    "mean=(0.485, 0.456, 0.406)\n",
    "std=(0.229, 0.224, 0.225)\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)])\n",
    "image = transform(pil_image).unsqueeze(0)\n",
    "print(image.shape)\n",
    "modules = list(model.modules())\n",
    "\n",
    "# Estrai i blocchi ViT (layers)\n",
    "vit_blocks = [module for module in modules if 'Block' in str(type(module))]\n",
    "# Example reshape (replace with actual values)\n",
    "B = 1\n",
    "N = 19\n",
    "C = 3\n",
    "\n",
    "# Reshape x to match (B, N, C)\n",
    "image = image.view(1, -1,192)\n",
    "\n",
    "\n",
    "tensor_image = vit_blocks[-2].attn(image)\n",
    "min_val = tensor_image.min()\n",
    "max_val = tensor_image.max()\n",
    "tensor_image = (tensor_image - min_val) / (max_val - min_val)\n",
    "\n",
    "# Converte il tensore in un'immagine PIL\n",
    "image = Image.fromarray((tensor_image[0] * 255).byte().numpy())\n",
    "\n",
    "# Mostra l'immagine\n",
    "plt.imshow(np.squeeze(image))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e9d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(CustomViT, self).__init__()\n",
    "        # Carica il modello ViT preaddestrato\n",
    "        self.model = timm.create_model(pretrained_model_name, pretrained=True)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        # Esegui la preparazione dei token qui\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.model.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # Aggiungi il token [CLS] ai token dei patch incorporati\n",
    "        cls_tokens = self.model.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Aggiungi la codifica posizionale a ciascun token\n",
    "        #x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.model.pos_drop(x)\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        # Ottieni l'ultimo livello di self-attention qui\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.model.blocks):\n",
    "            if i < len(self.model.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # Restituisci l'attenzione dell'ultimo blocco\n",
    "                attn_output = blk.attn(x)\n",
    "                attentions = attn_output\n",
    "    def forward(self, x):\n",
    "        # Esegui un passaggio in avanti utilizzando il modello preaddestrato\n",
    "        return self.model(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7b6ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'attention_maps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 40\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(att_mat\u001b[38;5;241m.\u001b[39mshape) \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_maps\n\u001b[0;32m---> 40\u001b[0m result1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_attention_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [15], line 37\u001b[0m, in \u001b[0;36mget_attention_map\u001b[0;34m(img, get_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m att_mat \u001b[38;5;241m=\u001b[39m model(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(att_mat\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattention_maps\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attention_maps' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "def get_attention_map(img, get_mask=False):\n",
    "    x = transform(img)\n",
    "    x.size()\n",
    "    \n",
    "\n",
    "    att_mat = model(x.unsqueeze(0))\n",
    "       \n",
    "    print(att_mat.shape) \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "    return attention_maps\n",
    "\n",
    "\n",
    "result1 = get_attention_map(pil_image, get_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8809f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.modules())\n",
    "\n",
    "# Estrai i blocchi ViT (layers)\n",
    "vit_blocks = [module for module in modules if 'Block' in str(type(module))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15cad77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " ),\n",
       " Block(\n",
       "   (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): Attention(\n",
       "     (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "     (q_norm): Identity()\n",
       "     (k_norm): Identity()\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): Identity()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (drop1): Dropout(p=0.0, inplace=False)\n",
       "     (norm): Identity()\n",
       "     (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (drop2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): Identity()\n",
       "   (drop_path2): Identity()\n",
       " )]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ade04344",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomViT(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit_tiny_patch16_224\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvisualize_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [50], line 8\u001b[0m, in \u001b[0;36mvisualize_predict\u001b[0;34m(model, img, img_size, patch_size, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_predict\u001b[39m(model, img, img_size, patch_size, device):\n\u001b[0;32m----> 8\u001b[0m     img_pre \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     attention \u001b[38;5;241m=\u001b[39m visualize_attention(model, img_pre, patch_size, device)\n\u001b[1;32m     10\u001b[0m     plot_attention(img, attention)\n",
      "\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model = CustomViT('vit_tiny_patch16_224')\n",
    "    \n",
    "visualize_predict(model, pil_image, (224,224), 16, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in5310",
   "language": "python",
   "name": "in5310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
